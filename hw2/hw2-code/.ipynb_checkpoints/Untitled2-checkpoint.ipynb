{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Softmax(object):\n",
    "\n",
    "  def __init__(self, dims=[10, 3073]):\n",
    "    self.init_weights(dims=dims)\n",
    "\n",
    "  def init_weights(self, dims):\n",
    "    \"\"\"\n",
    "    Initializes the weight matrix of the Softmax classifier.  \n",
    "    Note that it has shape (C, D) where C is the number of \n",
    "    classes and D is the feature size.\n",
    "    \"\"\"\n",
    "    self.W = np.random.normal(size=dims) * 0.0001\n",
    "\n",
    "  def loss(self, X, y):\n",
    "    \"\"\"\n",
    "    Calculates the softmax loss.\n",
    "  \n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "  \n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "  \n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the loss to zero.\n",
    "    loss = 0.0\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the normalized softmax loss.  Store it as the variable loss.\n",
    "    #   (That is, calculate the sum of the losses of all the training \n",
    "    #   set margins, and then normalize the loss by the number of \n",
    "    #   training examples.)\n",
    "    # ================================================================ #\n",
    "\n",
    "    m = X.shape[0]\n",
    "    for j in range(m):\n",
    "      z = self.W @ X[j]\n",
    "      e_x = np.exp(z - np.max(z))\n",
    "      softmax = e_x / np.sum(e_x)\n",
    "      loss += -np.log(softmax[y[j]])\n",
    "    loss /= m    \n",
    "\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "    Same as self.loss(X, y), except that it also returns the gradient.\n",
    "\n",
    "    Output: grad -- a matrix of the same dimensions as W containing \n",
    "      the gradient of the loss with respect to W.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    grad = np.zeros_like(self.W)\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the softmax loss and the gradient. Store the gradient\n",
    "    #   as the variable grad.\n",
    "    # ================================================================ #\n",
    "    m = X.shape[0]\n",
    "    for j in range(m):\n",
    "      z = self.W @ X[j]\n",
    "      e_x = np.exp(z - np.max(z))\n",
    "      softmax = e_x / np.sum(e_x)\n",
    "      loss += -np.log(softmax[y[j]])\n",
    "\n",
    "      for i in range(len(softmax)):\n",
    "        if (y[j] == i):\n",
    "          grad[i] += -(1-softmax[i]) * X[j]\n",
    "        else:\n",
    "          grad[i] += softmax[i] * X[j]\n",
    "        \n",
    "    loss /= m    \n",
    "    grad /= m\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def grad_check_sparse(self, X, y, your_grad, num_checks=10, h=1e-5):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in these dimensions.\n",
    "    \"\"\"\n",
    "  \n",
    "    for i in np.arange(num_checks):\n",
    "      ix = tuple([np.random.randint(m) for m in self.W.shape])\n",
    "  \n",
    "      oldval = self.W[ix]\n",
    "      self.W[ix] = oldval + h # increment by h\n",
    "      fxph = self.loss(X, y)\n",
    "      self.W[ix] = oldval - h # decrement by h\n",
    "      fxmh = self.loss(X,y) # evaluate f(x - h)\n",
    "      self.W[ix] = oldval # reset\n",
    "  \n",
    "      grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "      grad_analytic = your_grad[ix]\n",
    "      rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "      print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "\n",
    "  def fast_loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "    A vectorized implementation of loss_and_grad. It shares the same\n",
    "    inputs and ouptuts as loss_and_grad.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(self.W.shape) # initialize the gradient as zero\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the softmax loss and gradient WITHOUT any for loops.\n",
    "    # ================================================================ #\n",
    "    \n",
    "    z = X @ self.W.T\n",
    "    max_z = np.max(z, axis=1, keepdims=True)\n",
    "    e_x = np.exp(z - max_z)\n",
    "    e_x_sum = np.sum(e_x, axis=1, keepdims=True)\n",
    "    softmax = e_x / e_x_sum \n",
    "    loss = -np.log(np.choose(y, softmax.T))\n",
    "    loss = np.mean(loss)\n",
    "\n",
    "\n",
    "    softmax[np.arange(X.shape[0]),y] -= 1\n",
    "    grad += (X.T @ softmax).T\n",
    "    grad /= X.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "\n",
    "    self.init_weights(dims=[np.max(y) + 1, X.shape[1]])\t# initializes the weights of self.W\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "\n",
    "    for it in np.arange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Sample batch_size elements from the training data for use in \n",
    "      #     gradient descent.  After sampling,\n",
    "      #     - X_batch should have shape: (dim, batch_size)\n",
    "      #     - y_batch should have shape: (batch_size,)\n",
    "      #   The indices should be randomly generated to reduce correlations\n",
    "      #   in the dataset.  Use np.random.choice.  It's okay to sample with\n",
    "      #   replacement.\n",
    "      # ================================================================ #\n",
    "      idx = np.random.randint(X.shape[0], size=batch_size)\n",
    "      X_batch = X[idx, :]\n",
    "      y_batch = y[idx]\n",
    "\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.fast_loss_and_grad(X_batch, y_batch)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Update the parameters, self.W, with a gradient step \n",
    "      # ================================================================ #\n",
    "      self.W = self.W - learning_rate * grad\n",
    "\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Predict the labels given the training data.\n",
    "    # ================================================================ #\n",
    "    z = X @ self.W.T\n",
    "    max_z = np.max(z, axis=1, keepdims=True)\n",
    "    e_x = np.exp(z - max_z)\n",
    "    e_x_sum = np.sum(e_x, axis=1, keepdims=True)\n",
    "    softmax = e_x / e_x_sum \n",
    "    y_pred = np.argmax(softmax, axis=1)\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return y_pred\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
